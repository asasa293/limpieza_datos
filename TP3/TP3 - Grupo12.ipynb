{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 Grupo 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando librerías  \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para evaluación del modelo con regresión logística, variando datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model(X_train, X_test, y_train, y_test):\n",
    "    '''Creacióm del modelo de Regresión Logistica variando datos de entrada'''\n",
    "    \n",
    "    lr = LogisticRegression(C=1.0,penalty='l2',random_state=1,solver=\"newton-cholesky\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_resultados(y_test, y_pred):\n",
    "    confusion_matrix(y_test, y_test_pred)\n",
    "    #plt.figure(figsize=(12, 12))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='.0f');\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('Verdaderos')\n",
    "    plt.xlabel('Predichos')\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lectura de DataFrame "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DataFrame usado para este TP consta de datos provenientes de la plataforma Kaggle, en el que se evalúan algunas variables relacionadas con evaluación de posible fraude en transacciones bancarias realizadas con tarjetas de crédito. Se trata de un Dataset desbalanceado, respecto a la proporción en la variable target (fraud). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'card_transdata.csv', sep=',')\n",
    "print(f'El dataframe de transacciones está compuesto por {data.shape[1]} columnas y {data.shape[0]} filas') \n",
    "print(data.sample(3))\n",
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descripción breve del DataFrame.\n",
    "\n",
    "\n",
    "El DataFrame en estudio cuenta con 8 columnas y un total de 1000000 filas (no tiene valores nulos). \n",
    "\n",
    "Las columnas contenidas en el DataFrame son: \n",
    "\n",
    "2.1 distance_from_home. Indica la distancia desde casa respecto al lugar en donde ocurrió la transacción. Ejemplo: 2.813104. \n",
    "\n",
    "2.2 distance_from_last_transaction. Indica la distancia entre el punto de la transacción con respecto a la última transacción registrada. Ejemplo: 0.153290.\n",
    "\n",
    "2.3 ratio_to_median_purchase_price. es la relación entre el monto de la transacción y el precio medio de compra del cliente. Ejemplo: 1.135134.\n",
    "\n",
    "2.4 repeat_retailer. Columna con variables dummies para indicar con 1 si la transacción se realizó desde el mismo minorista y con 0 si no. Ejemplo: 1.0.\n",
    "\n",
    "2.5 used_chip. Columna con variables dummies para indicar con 1 si en la transacción se utilizó el chip de la tarjeta de crédito y con 0 si no se utilizó. Ejemplo: 0.0.\n",
    "\n",
    "2.6 used_pin_number. Columna con variables dummies que indica con 1 si en la transacción se usó el número de pin y con 0 en caso contrario. Ejemplo: 1.0.\n",
    "\n",
    "2.7 online_order. Columna con variables dummies en la que se indica con 1 si la transacción corresponde a un pedido en línea y 0 si no fue así. Ejemplo: 1.0. \n",
    "\n",
    "2.8 fraud. Columna con variable target que indica si la transacción fue identificada como fraude (1) o no (0). Ejemplo: 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de nombre y tipo de dato por columna\n",
    "\n",
    "print('\\n')\n",
    "print('-----------Tipo de datos por columna-----------')\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribución de los datos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber valores no nulos\n",
    "\n",
    "msno.bar(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del DataFrame no se encuentran valores nulos, todas las variables a usar en los diferentes modelos (KNN, Naives Bayes y Regresión Logística) tienen los los datos completos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Variable TARGET a Determinar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el análisis realizado y la propuesta original en el dataset, la variable target a considerar, en la construcción de los diferentes modelos,  será la columna \"fraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerando lo anterior, resulta necesario conocer la proporción de la variable target\n",
    "count_classes = data.fraud.value_counts(sort=True)\n",
    "count_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una población de 1000000 de transacciones, tan sólo 87403 son etiquetadas como casos de fraude. Representan aproximadamente el 9% de las muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Frecuencia de numero de observaciones\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Numero de observaciones\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propuestas \n",
    "\n",
    "1. Trabajar con dataset total. (1MM de registros, proporción variable target 90-10)\n",
    "2. Seleccionar menor cantidad de registros y buscando una proporción de la variable target 70-30, para tener el datasets menos desbalanceado. Según cálculos , se debería extraer una muestra de 291343 datos totales. \n",
    "\n",
    "\n",
    "A partir de la selección del dataset: \n",
    "\n",
    "1. Aplicar los siguientes modelos sobre el conjunto de datos: \n",
    "\n",
    "   1.a. KNN. (Carlos)\n",
    "\n",
    "   1.b. Naives Bayes. (Santiago)\n",
    "   \n",
    "   1.c. Regresión Logística. (Moises) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de sólo no fraude \n",
    "df_nf = df.loc[df['fraud'] == 0]\n",
    "df_nf.fraud.value_counts()\n",
    "df_nf2 = df_nf.sample(203940)\n",
    "print(df_nf2.fraud.value_counts())\n",
    "\n",
    "# Generar otro dataset con los registros de fraude \n",
    "df_f = df.loc[df['fraud'] == 1]\n",
    "df_f.fraud.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.merge(df_nf2, df_f, how = 'outer')\n",
    "datos.fraud.value_counts()\n",
    "\n",
    "display(datos.sample(10))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aplicando modelo de Regresión Logística"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Full Datos. \n",
    "    \n",
    "    Como 1era aproximación para la generación del modelo, será usado todo el volumne de datos. Sabiendo que la variable a predecir se encuentra desbalanceada.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para considerar las variables predictoras\n",
    "\n",
    "feature_cols = [x for x in data.columns if ((x != 'fraud'))]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de los datos en train y test\n",
    "# ==============================================================================\n",
    "X = data[feature_cols]\n",
    "y = data['fraud']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X,\n",
    "                                        y,\n",
    "                                        stratify=y,\n",
    "                                        train_size   = 0.8,\n",
    "                                        random_state = 123,\n",
    "                                     )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenando al modelo \n",
    "# Instanciamos un objeto de esa clase\n",
    "logistic_regression = LogisticRegression()\n",
    "# Ajustamos esta instancia con los datos de entrenamiento\n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "print(logistic_regression.coef_)\n",
    "print(logistic_regression.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos predicciones con el modelo entrenado\n",
    "y_train_pred = logistic_regression.predict(X_train_scaled)\n",
    "y_test_pred = logistic_regression.predict(scaler.transform(X_test)) # Notar que debemos escalar los datos de testeo antes de realizar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elaboramos la matriz de confusión\n",
    "confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='.0f')\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.xlabel('Predichos');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos el accuracy\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la sensibilidad del modelo\n",
    "sensibilidad = recall_score(y_test, y_test_pred)\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntajef1 = f1_score(y_test, y_test_pred)\n",
    "print('Puntaje F1 del modelo:')\n",
    "print(puntajef1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "print('Curva ROC - AUC del modelo:')\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haciendo un resumen de los parámetros obtenidos usando todo el dataset \n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la matriz de confusión se obtienen alrededor de 7000 fallos y 10000 aciertos dando un recall de 0.60 y es el valor que queremos mejorar. También es interesante notar que en la columna de f1-score obtenemos muy buenos resultados, a pesar de ello deben ser considerados con ciertas reservas, pues están reflejando una realidad parcial. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas métricas serán consideradas como línea base para comparación y búsqueda de mejora. Un aspecto a evaluar será el tema del hecho que el dataset se encuentra desbalanceado. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Buscando balancear las clases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Penalización de clases mayoritarias\n",
    "\n",
    "Según literatura de LogisticRegression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "- class_weight: dict or ‘balanced’, default=None\n",
    "Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
    "\n",
    "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "\n",
    "Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n",
    "\n",
    "- solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "Algorithm to use in the optimization problem. Default is ‘lbfgs’. To choose a solver, you might want to consider the following aspects:\n",
    "‘newton-cholesky’ is a good choice for n_samples >> n_features, especially with one-hot encoded categorical features with rare categories. Note that it is limited to binary classification and the one-versus-rest reduction for multiclass classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalidad = LogisticRegression(C=1.0,penalty='l2',random_state=1,solver=\"newton-cholesky\",class_weight=\"balanced\")\n",
    "penalidad.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = penalidad.predict(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_resultados(y_test, y_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Undersampling (us) sobre la clase predominante (No Fraude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us trabaja parecido a KNN\n",
    "us = NearMiss(n_neighbors=3, version=1)\n",
    "# Para cambiar proporciones podemos usar version = 3\n",
    "\n",
    "X_train_res, y_train_res = us.fit_resample(X_train, y_train)\n",
    " \n",
    "print(f'Distribución antes de resampling {Counter(y_train)}')\n",
    "print(f'Distribución antes de resampling {Counter(y_train_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic_model(X_train_res, X_test, y_train_res, y_test)\n",
    "y_test_pred = model.predict(scaler.transform(X_test))\n",
    "\n",
    "mostrar_resultados(y_test, y_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifica todos los fraudes porque la probabilidad en el set de entrenamiento la proporción es 50/50. Aumentan los falsos positivos, disminuye el f1 score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Oversampling (os) de la clase minoritaria (Fraude)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso se usará RandomOverSampler para crear nuevas muestras \"sintéticas\" para la clase 'Fraude'. \n",
    "Al usar 'sampling_strategy' = 0.5, estamos creando muestras sintéticas hasta alcanzar un numero de muestras igual al 50% de la clase mayoritaria (No Fraude), es decir 365039 casos Fraude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os =  RandomOverSampler(sampling_strategy = 0.5, random_state=1234)\n",
    "X_train_res, y_train_res = os.fit_resample(X_train, y_train)\n",
    " \n",
    "print(f'Distribución antes de resampling {Counter(y_train)}')\n",
    "print(f'Distribución después de resampling {Counter(y_train_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic_model(X_train_res, X_test, y_train_res, y_test)\n",
    "y_test_pred = model.predict(scaler.transform(X_test))\n",
    "\n",
    "mostrar_resultados(y_test, y_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentan los falsos negativos, disminuyen los falsos positivos, aumenta la precisión y el F1 score. Se identifican menor cantidad de casos de Fraude que el caso anterior. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Aplicando modelos conjuntos. (Oversampling (os) y Undersampling (us)) -- Smote-Tomek "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SMOTE (Synthetic Minority Over-sampling Technique) para oversampling. Puntos vecinos cercanos y agrega puntos “en linea recta” entre ellos. \n",
    "\n",
    "* Tomek para undersampling. Elimina los de distinta clase que sean nearest neighbor y deja ver mejor el decisión boundary (la zona limítrofe de las clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy=0.5, random_state=1234)\n",
    "X_train_res, y_train_res = os_us.fit_resample(X_train, y_train)\n",
    " \n",
    "print(f'Distribución antes de resampling {Counter(y_train)}')\n",
    "print(f'Distribución antes de resampling {Counter(y_train_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic_model(X_train_res, X_test, y_train_res, y_test)\n",
    "y_test_pred = model.predict(scaler.transform(X_test))\n",
    "\n",
    "mostrar_resultados(y_test, y_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentan los falsos negativos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aplicando modelo Naiyes Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAMOS A HACER UN PAIR PLOT PARA VER SI LOS DATOS ESTÁN BIEN DIFERENCIADOS ENTRE CLASES.\n",
    "# PARA MAYOR VELOCIDAD AL GRAFICAR, USAREMOS UN DF MUY PEQUEÑO.\n",
    "# PARA ENTRENAR EL MODELO SE USARÁ UNO MÁS GRANDE\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.99,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE IGUAL FORMA, SE ESCALARÁN LOS DATOS PARA VISUALIZAR MEJOR EL PAIR PLOT.\n",
    "ss = StandardScaler()\n",
    "xscaled = pd.DataFrame(ss.fit_transform(x_train),columns = x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAS CLASES PARECEN ESTAR BIEN SEPARADAS PARA ALGUNAS VARIABLES \n",
    "# POR LO QUE ES VIABLE HACER UN MODELO DE CLASIFICACIÓN.\n",
    "pd.plotting.scatter_matrix(xscaled, c=y_train, figsize=(20,20),\n",
    "                           marker='o', )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Naive Bayes (Todos los datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOLVEMOS A SEPARAR EN TRAIN Y TEST PERO ESTA VEZ\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=123,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GaussianNB()\n",
    "model1.fit(x_train, y_train); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultinomialNB()\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gnb = model1.predict(x_test)\n",
    "y_pred_mnb = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Accuracy para modelo gaussiano: {accuracy_score(y_pred_gnb,y_test)}')\n",
    "print (f'Accuracy para modelo multinomial: {accuracy_score(y_pred_mnb,y_test)}')\n",
    "print()\n",
    "print (f'Recall para modelo gaussiano: {recall_score(y_test,y_pred_gnb)}')\n",
    "print (f'Recall para modelo multinomial: {recall_score(y_test,y_pred_mnb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.heatmap(confusion_matrix(y_test,model1.predict(x_test)),annot=True, fmt='.0f')\n",
    "g.set_xlabel('predicho')\n",
    "g.set_ylabel('real')\n",
    "g.set_title('GaussianNB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.heatmap(confusion_matrix(y_test,model2.predict(x_test)),annot=True, fmt='.2f')\n",
    "g.set_xlabel('predicho')\n",
    "g.set_ylabel('real')\n",
    "g.set_title('MultinomialNB')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Naive Bayes (Balanceado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para este modelo sólo será construido un DataFrame considerando la siguiente premisa: \n",
    "* Seleccionar menor cantidad de registros y buscando una proporción de la variable target 70-30, para tener el datasets menos desbalanceado. Según cálculos , se debería extraer una muestra de 291343 datos totales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de sólo no fraude \n",
    "df_nf = data.loc[df['fraud'] == 0]\n",
    "df_nf.fraud.value_counts()\n",
    "df_nf2 = df_nf.sample(203940, random_state=123)\n",
    "print(df_nf2.fraud.value_counts())\n",
    "\n",
    "# Generar otro dataset con los registros de fraude \n",
    "df_f = data.loc[df['fraud'] == 1]\n",
    "df_f.fraud.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitivo = pd.merge(df_nf2, df_f, how = 'outer')\n",
    "definitivo.fraud.value_counts()\n",
    "\n",
    "display(definitivo.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = definitivo.iloc[:, 0:-1]\n",
    "y = definitivo.iloc[:, -1]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=123,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GaussianNB()\n",
    "model1.fit(x_train, y_train);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultinomialNB()\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gnb = model1.predict(x_test)\n",
    "y_pred_mnb = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Accuracy para modelo gaussiano: {accuracy_score(y_pred_gnb,y_test)}')\n",
    "print (f'Accuracy para modelo multinomial: {accuracy_score(y_pred_mnb,y_test),}')\n",
    "print()\n",
    "print (f'Recall para modelo gaussiano: {recall_score(y_test,y_pred_gnb)}')\n",
    "print (f'Recall para modelo multinomial: {recall_score(y_test,y_pred_mnb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.heatmap(confusion_matrix(y_test,model1.predict(x_test)),annot=True, fmt='.2f')\n",
    "g.set_xlabel('predicho')\n",
    "g.set_ylabel('real')\n",
    "g.set_title('GaussianNB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.heatmap(confusion_matrix(y_test,model2.predict(x_test)),annot=True, fmt='.2f')\n",
    "g.set_xlabel('predicho')\n",
    "g.set_ylabel('real')\n",
    "g.set_title('MultinomialNB')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aplicando modelo KNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. KNN (Todos los datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos la matriz de features donde tomamos todo el DataFrame en la eje X\n",
    "X = data.iloc[:, 0:-1]\n",
    "# Construimos el vector target Y con la columna Fraud del DF\n",
    "y = data['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serán generados los datos de entrenamiento y test\n",
    "# El argumento stratify nos permite generar una división que respeta la misma proporción entre clases en ambos sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificando el dataframe con el modelo KNN \n",
    "# Inicialmente tomamos el hiperparametro por defecto, sin estandarizar la matriz\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "# Ajustamos a los datos de entrenamiento\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos etiquetas para los datos de test\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entre los datos de pruebas y prediccion, vemos que se obtiene un 98% de Precision de conincidencia entre la muestra real y la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el accuracy del modelo\n",
    "accuracy_score(y_test, y_pred).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizamos el valor del hiperparametro K, haciendo unsando recall_score tomando un rango de 1 a 10 para posterioirmente determinar el valor optimo en K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizar el valor de K\n",
    "\n",
    "scores_para_df = []\n",
    "\n",
    "for i in tqdm(range(1,10)):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    dict_row_score = {'recall': recall_score(y_test, y_pred), 'n_neighbors':i}\n",
    "    # Guardamos cada uno en la lista de diccionarios\n",
    "    scores_para_df.append(dict_row_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(scores_para_df)\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los resultados\n",
    "plt.plot(df_scores['n_neighbors'], df_scores['recall'], color='g');\n",
    "plt.xlabel('n_neighbors'); plt.ylabel('Recall')\n",
    "plt.grid(); plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validamos cual es el valor que maximiza el score de recall, siendo en este caso el n_neighbors= 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Estandarizacion de los datos de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estandarizacion Matriz\n",
    "# Utilizamos sklearn para estandarizar la matriz de features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que las variables ahora tengan media 0 y desvío 1.\n",
    "print('Medias:', np.mean(X_train, axis=0).round(2))\n",
    "print('Desvio:', np.std(X_train, axis=0).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos nuevamente los scores de Recall pero esta vez sobre los features estandarizados:\n",
    "\n",
    "scores_para_df_standard = []\n",
    "\n",
    "for i in tqdm(range(1,10)):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    dict_row_score = {'recall': recall_score(y_test, y_pred), 'n_neighbors':i}\n",
    "    # Guardamos cada uno en la lista de diccionarios\n",
    "    scores_para_df_standard.append(dict_row_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el DataFrame a partir de la lista de diccionarios\n",
    "df_scores_standard = pd.DataFrame(scores_para_df_standard)\n",
    "df_scores_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los resultados\n",
    "plt.plot(df_scores_standard['n_neighbors'], df_scores_standard['recall'], color='g');\n",
    "plt.xlabel('n_neighbors'); plt.ylabel('Recall')\n",
    "plt.grid(); plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora obteniendo el valor optimo, conseguido en el paso anterioir, hacemos de nuevo el entramiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos el valor del k óptimo a una variable\n",
    "best_k = df_scores_standard.loc[df_scores.recall == df_scores.recall.max(), 'n_neighbors'].values[0]\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos el modelo óptimo de acuerdo a las pruebas de recall_score\n",
    "model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "\n",
    "# Lo ajustamos sobre los datos de entrenamiento\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un ajuste cerca de 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos qué accuracy obtenemos en train\n",
    "accuracy_score(y_train, model.predict(X_train)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.heatmap(cm, annot=True, fmt='.0f')\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicción');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Casos negativos reales 62.397, correspondientes a la primera fila.\n",
    "\n",
    "* Casos positivos reales 14.318, correspondientes a la segunda fila.\n",
    "\n",
    "* Casos negativos predichos, corresponden a la primera columna.\n",
    "\n",
    "* Casos positivos predichos, corresponden a la segunda columna.\n",
    "\n",
    "* Diagonal en la matriz corresponde a las clasificaciones correctas. Siendo 62.397 casos negativos y 14.318 a casos positivos de Fraude.\n",
    "Los Elementos de la diagonal inversa, nuestro modelo los confude siendo 165.752 casos mal etiquetado para negativo y 7.533 para positivo de fraude."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finalmente calculamos la exactitud de nuestro modelo, de manera manual y computada, siendo ambas de precision del 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((cm[0,0] + cm[1,1]) / len(y_pred)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Balanceo de clases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partiendo desde la misma premisa considerada para Naives Bayes, será usado el DataFrame generado de manera manual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar matriz de *features* y vector *target*\n",
    "X = definitivo.iloc[: , :7]\n",
    "y = definitivo['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir los hiperparámetros del modelo. Instanciamos el modelo con su configuración por defecto\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar los sets de entrenamiento y de testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=2)\n",
    "\n",
    "# Ajustamos a los datos de entrenamiento.\n",
    "knn.fit(X_train, y_train);\n",
    "\n",
    "# Predecimos etiquetas para los datos de test.\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluamos el accuracy del modelo\n",
    "accuracy_score(y_test, y_pred).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nuestro modelo puede clasificar correctamente el 97% de las observaciones del dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma 1.\n",
    "# # Ahora, vamos a evaluar el modelo\n",
    "\n",
    "X_train_train, X_validation, y_train_train, y_validation = train_test_split(X_train, y_train, random_state=2)\n",
    "\n",
    "k_range = (range(1, 25))\n",
    "scores = []\n",
    "for k in tqdm(k_range):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_train, y_train_train)\n",
    "    y_pred = knn.predict(X_validation)\n",
    "    scores.append(accuracy_score(y_validation, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando la mejor busqueda del hiperparámetro\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Valor de K'); plt.ylabel('Test Accuracy')\n",
    "plt.grid(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma 2.\n",
    "# Ahora, vamos a evaluar el modelo\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "scores_para_definitivo = []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=kf)\n",
    "\n",
    "    dict_row_score = {'score_medio':np.mean(cv_scores),\n",
    "                      'score_std':np.std(cv_scores), 'n_neighbors':i}\n",
    "    \n",
    "    scores_para_definitivo.append(dict_row_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el DataFrame a partir de la lista de diccionarios\n",
    "df_scores = pd.DataFrame(scores_para_definitivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos los límites inferior y superior\n",
    "df_scores['limite_inferior'] = df_scores['score_medio'] - df_scores['score_std']\n",
    "df_scores['limite_superior'] = df_scores['score_medio'] + df_scores['score_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los resultados\n",
    "plt.plot(df_scores['n_neighbors'], df_scores['limite_inferior'], color='r')\n",
    "plt.plot(df_scores['n_neighbors'], df_scores['score_medio'], color='b')\n",
    "plt.plot(df_scores['n_neighbors'], df_scores['limite_superior'], color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificamos el score máximo\n",
    "df_scores.loc[df_scores.score_medio == df_scores.score_medio.max()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6becbed91c3db0de89d3be7d755cbeeac818c5017507b382c86305c080f557e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
